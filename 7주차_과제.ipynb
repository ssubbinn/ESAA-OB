{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnksJO0uct4Ws4uZJcySgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssubbinn/ESAA-OB/blob/main/7%EC%A3%BC%EC%B0%A8_%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토픽 모델링 - 20 뉴스그룹\n",
        "\n",
        "LSA, LDA(latent dirichlet allocation)\n",
        "\n",
        "- 20 뉴스그룹은 20가지 주제를 가진 뉴스그룹의 데이터를 가짐\n",
        "- 이중 주제 8개만 추출하여 모델링 적용\n"
      ],
      "metadata": {
        "id": "fFVzjIJIifDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#8개 주제 추출\n",
        "cats=['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
        "      'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
        "\n",
        "# 위에서 cats 변수로 기재된 카테고리만 추출. featch_20newsgroups()의 categories에 cats 입력\n",
        "news_df=fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), categories=cats, random_state=0)\n",
        "\n",
        "#LDA Count 기반의 벡터화만 적용합니다\n",
        "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
        "\n",
        "feat_vect=count_vect.fit_transform(news_df.data)\n",
        "print('CountVectorizer Shape:', feat_vect.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyyBDJRTkC9S",
        "outputId": "66fe6788-2f66-4be0-d03a-1b300f90d122"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer Shape: (7862, 1000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lda=LatentDirichletAllocation(n_components=8, random_state=0)\n",
        "lda.fit(feat_vect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "6Hbe2lHllX6I",
        "outputId": "59eb56dc-a0b0-4bd2-9de8-9cf006511b39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(n_components=8, random_state=0)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lda.components_.shape)\n",
        "lda.components_\n",
        "\n",
        "# 개별 토픽별로 각 word 피처가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치-> 높으수록 해당 word 피처는 그 토픽의 중심 word가 됨됨"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb-Gm0Lp6iUy",
        "outputId": "b990b3d2-d97a-4833-a48a-b1b4a0cb713f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 1000)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
              "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
              "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
              "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
              "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
              "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
              "       ...,\n",
              "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
              "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
              "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
              "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
              "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
              "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#각 토픽별로 연관도가 높은 순으로 word를 나열\n",
        "\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_index, topic in enumerate(model.components_):\n",
        "        print('Topic #',topic_index)\n",
        "\n",
        "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
        "        topic_word_indexes = topic.argsort()[::-1]\n",
        "        top_indexes=topic_word_indexes[:no_top_words]\n",
        "        \n",
        "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
        "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])                \n",
        "        print(feature_concat)\n",
        "\n",
        "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "\n",
        "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
        "display_topics(lda, feature_names, 15)"
      ],
      "metadata": {
        "id": "gPRu915RzpeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247d543c-2c6b-4adf-957a-71d4fbdc6039"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic # 0\n",
            "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
            "Topic # 1\n",
            "don just like know people said think time ve didn right going say ll way\n",
            "Topic # 2\n",
            "image file jpeg program gif images output format files color entry 00 use bit 03\n",
            "Topic # 3\n",
            "like know don think use does just good time book read information people used post\n",
            "Topic # 4\n",
            "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
            "Topic # 5\n",
            "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
            "Topic # 6\n",
            "god people jesus church believe christ does christian say think christians bible faith sin life\n",
            "Topic # 7\n",
            "use dos thanks windows using window does display help like problem server need know run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "topic 0,5,7 애매한 주제어 추출"
      ],
      "metadata": {
        "id": "OctV1AjL9Abr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 문서 유사도"
      ],
      "metadata": {
        "id": "4oJOxZ-w9NtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX5d3rmE9RPw",
        "outputId": "5d8d7fe9-8713-4e82-9073-bd2834712d8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cos_similarity(v1, v2):\n",
        "    dot_product = np.dot(v1, v2)\n",
        "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
        "    similarity = dot_product / l2_norm     \n",
        "    \n",
        "    return similarity"
      ],
      "metadata": {
        "id": "JCYAtMtx-K3D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "doc_list = ['if you take the blue pill, the story ends' ,\n",
        "            'if you take the red pill, you stay in Wonderland',\n",
        "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
        "\n",
        "tfidf_vect_simple = TfidfVectorizer()\n",
        "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
        "print(feature_vect_simple.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6RVcMSf-L8S",
        "outputId": "65ad877a-e6fc-46ce-e2f3-2bd9735524f9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TFidfVectorizer로 transform()한 결과는 Sparse Matrix이므로 Dense Matrix로 변환. \n",
        "feature_vect_dense = feature_vect_simple.todense()\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector  추출\n",
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "\n",
        "#첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출\n",
        "similarity_simple = cos_similarity(vect1, vect2 )\n",
        "print('문장 1, 문장 2 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9PZZe0r-NYh",
        "outputId": "3122a254-4443-48de-a4fb-b5af90ce26bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 2 Cosine 유사도: 0.402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect1, vect3 )\n",
        "print('문장 1, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))\n",
        "\n",
        "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
        "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
        "similarity_simple = cos_similarity(vect2, vect3 )\n",
        "print('문장 2, 문장 3 Cosine 유사도: {0:.3f}'.format(similarity_simple))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPmhkW77-PEf",
        "outputId": "32ecf1ed-1c1f-434d-ce04-3796ad0cb4be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 1, 문장 3 Cosine 유사도: 0.404\n",
            "문장 2, 문장 3 Cosine 유사도: 0.456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple)\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdhMlC7C-QH0",
        "outputId": "5f74c0b2-e3d9-4aa5-f45c-cfab51b04b46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple[1:])\n",
        "print(similarity_simple_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6YgXrJc-RdD",
        "outputId": "b3b3d388-350f-459f-9dd2-88abb64a0276"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.40207758 0.40425045]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_simple_pair = cosine_similarity(feature_vect_simple , feature_vect_simple)\n",
        "print(similarity_simple_pair)\n",
        "print('shape:',similarity_simple_pair.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoraTDbK-T9e",
        "outputId": "ec362f30-0630-4f0e-e5ad-430c3140b778"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.40207758 0.40425045]\n",
            " [0.40207758 1.         0.45647296]\n",
            " [0.40425045 0.45647296 1.        ]]\n",
            "shape: (3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Opinion Review 데이터 셋을 이용한 문서 유사도 측정"
      ],
      "metadata": {
        "id": "LLEF_FbN-ZWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "lemmar = WordNetLemmatizer()\n",
        "\n",
        "# 입력으로 들어온 token단어들에 대해서 lemmatization 어근 변환. \n",
        "def LemTokens(tokens):\n",
        "    return [lemmar.lemmatize(token) for token in tokens]\n",
        "\n",
        "# TfidfVectorizer 객체 생성 시 tokenizer인자로 해당 함수를 설정하여 lemmatization 적용\n",
        "# 입력으로 문장을 받아서 stop words 제거-> 소문자 변환 -> 단어 토큰화 -> lemmatization 어근 변환. \n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "zX-qDcB6-a7I"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_T0IGx0_lO2",
        "outputId": "b250ccd2-769b-416d-f4e3-3541d392dd0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZISgX2KAacG",
        "outputId": "d3f48367-57ed-4db9-eb27-a1cb745b853a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlrF2EuQAgXs",
        "outputId": "752946fc-55b4-4271-ffdd-fb63e224ba03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import os"
      ],
      "metadata": {
        "id": "5rQa-6g0CNyD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Rmc1gREzCPfX",
        "outputId": "11f466d7-cb9a-4d67-880d-57af477db7ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob, os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "path = r'C:\\OpinosisDataset1.0\\OpinosisDataset1.0\\topics'\n",
        "all_files = glob.glob(os.path.join(path, \"*.data\"))     \n",
        "filename_list = []\n",
        "opinion_text = []\n",
        "\n",
        "for file_ in all_files:\n",
        "    df = pd.read_table(file_,index_col=None, header=0,encoding='latin1')\n",
        "    filename_ = file_.split('\\\\')[-1]\n",
        "    filename = filename_.split('.')[0]\n",
        "    filename_list.append(filename)\n",
        "    opinion_text.append(df.to_string())\n",
        "\n",
        "document_df = pd.DataFrame({'filename':filename_list, 'opinion_text':opinion_text})\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english' , \\\n",
        "                             ngram_range=(1,2), min_df=0.05, max_df=0.85 )\n",
        "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
        "\n",
        "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
        "km_cluster.fit(feature_vect)\n",
        "cluster_label = km_cluster.labels_\n",
        "cluster_centers = km_cluster.cluster_centers_\n",
        "document_df['cluster_label'] = cluster_label"
      ],
      "metadata": {
        "id": "H57D330ZDgVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# cluster_label=2인 데이터는 호텔로 클러스터링된 데이터임. DataFrame에서 해당 Index를 추출\n",
        "hotel_indexes = document_df[document_df['cluster_label']==2].index\n",
        "print('호텔로 클러스터링 된 문서들의 DataFrame Index:', hotel_indexes)\n",
        "\n",
        "# 호텔로 클러스터링된 데이터 중 첫번째 문서를 추출하여 파일명 표시.  \n",
        "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
        "print('##### 비교 기준 문서명 ',comparison_docname,' 와 타 문서 유사도######')\n",
        "\n",
        "''' document_df에서 추출한 Index 객체를 feature_vect로 입력하여 호텔 클러스터링된 feature_vect 추출 \n",
        "이를 이용하여 호텔로 클러스터링된 문서 중 첫번째 문서와 다른 문서간의 코사인 유사도 측정.'''\n",
        "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]] , feature_vect[hotel_indexes])\n",
        "print(similarity_pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLoJeG0Y-fcX",
        "outputId": "796af9e3-43c5-4ff0-b99b-75576b75ca95"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "호텔로 클러스터링 된 문서들의 DataFrame Index: Int64Index([0, 4, 5, 9, 14, 17, 18, 29, 34, 36, 38, 39, 40, 43, 45, 50], dtype='int64')\n",
            "##### 비교 기준 문서명  /content/drive/MyDrive/김수빈/ESAA/OB/data/OpinosisDataset1  와 타 문서 유사도######\n",
            "[[1.         0.11742762 0.05846178 0.05221059 0.03638665 0.06189595\n",
            "  0.06193118 0.0430688  0.13989623 0.1386783  0.51442299 0.07049362\n",
            "  0.11282857 0.32619948 0.09518068 0.38038865]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# argsort()를 이용하여 앞예제의 첫번째 문서와 타 문서간 유사도가 큰 순으로 정렬한 인덱스 반환하되 자기 자신은 제외. \n",
        "sorted_index = similarity_pair.argsort()[:,::-1]\n",
        "sorted_index = sorted_index[:, 1:]\n",
        "\n",
        "# 유사도가 큰 순으로 hotel_indexes를 추출하여 재 정렬. \n",
        "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
        "\n",
        "# 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외\n",
        "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
        "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
        "\n",
        "# 유사도가 큰 순으로 정렬된 Index와 유사도값을 이용하여 파일명과 유사도값을 Seaborn 막대 그래프로 시각화\n",
        "hotel_1_sim_df = pd.DataFrame()\n",
        "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
        "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
        "\n",
        "fig1 = plt.gcf()\n",
        "sns.barplot(x='similarity', y='filename',data=hotel_1_sim_df)\n",
        "plt.title(comparison_docname)\n",
        "fig1.savefig('p553_hotel.tif', format='tif', dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "2zXOBViZDikP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. 한글 텍스트 처리-네이버 영화 평점 감성 분석석"
      ],
      "metadata": {
        "id": "4ahyRINgD0H1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wx_dnGUD4R-",
        "outputId": "71c14f60-d166-40ed-c83c-e66ed404c235"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ratings_test.txt', <http.client.HTTPMessage at 0x7ff699f9da80>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_table('ratings_train.txt')\n",
        "test_df = pd.read_table('ratings_test.txt')"
      ],
      "metadata": {
        "id": "IStfmJMkEkhF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "9BprvwlHEmir",
        "outputId": "52580818-bab0-45fa-e2cf-00b116cb8590"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id                           document  label\n",
              "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-529e17fe-43b0-4ff1-b46e-fcb708f3247b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-529e17fe-43b0-4ff1-b46e-fcb708f3247b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-529e17fe-43b0-4ff1-b46e-fcb708f3247b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-529e17fe-43b0-4ff1-b46e-fcb708f3247b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['label'].value_counts( )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg8LkUwUEqiK",
        "outputId": "eb71a2b8-2497-4ca2-f3f7-6a681119fb45"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75173\n",
              "1    74827\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "train_df = train_df.fillna(' ')\n",
        "# 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 \\d 는 숫자를 의미함.) \n",
        "train_df['document'] = train_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
        "\n",
        "# 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환\n",
        "test_df = pd.read_csv('ratings_test.txt', sep='\\t')\n",
        "test_df = test_df.fillna(' ')\n",
        "test_df['document'] = test_df['document'].apply( lambda x : re.sub(r\"\\d+\", \" \", x) )\n",
        "\n",
        "# id 칼럼 삭제 수행\n",
        "train_df.drop('id', axis=1, inplace=True) \n",
        "test_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "wSRLzbMwEtlF"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Twitter\n",
        "\n",
        "twitter = Twitter()\n",
        "def tw_tokenizer(text):\n",
        "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
        "    tokens_ko = twitter.morphs(text)\n",
        "    return tokens_ko"
      ],
      "metadata": {
        "id": "kUZAYpKDE9bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Twitter 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2) \n",
        "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9)\n",
        "tfidf_vect.fit(train_df['document'])\n",
        "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
      ],
      "metadata": {
        "id": "cW9_eNJJE-3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression 을 이용하여 감성 분석 Classification 수행. \n",
        "lg_clf = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Parameter C 최적화를 위해 GridSearchCV 를 이용. \n",
        "params = { 'C': [1 ,3.5, 4.5, 5.5, 10 ] }\n",
        "grid_cv = GridSearchCV(lg_clf , param_grid=params , cv=3 ,scoring='accuracy', verbose=1 )\n",
        "grid_cv.fit(tfidf_matrix_train , train_df['label'] )\n",
        "print(grid_cv.best_params_ , round(grid_cv.best_score_,4))"
      ],
      "metadata": {
        "id": "iyWRxKEWEx1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 학습 데이터를 적용한 TfidfVectorizer를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함. \n",
        "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
        "\n",
        "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
        "best_estimator = grid_cv.best_estimator_\n",
        "preds = best_estimator.predict(tfidf_matrix_test)\n",
        "\n",
        "print('Logistic Regression 정확도: ',accuracy_score(test_df['label'],preds))"
      ],
      "metadata": {
        "id": "xI2admK8Ey-w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}