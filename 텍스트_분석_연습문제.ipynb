{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssubbinn/ESAA-OB/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%B6%84%EC%84%9D_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 텍스트 분석 연습 문제**\n",
        "\n",
        "- 출처 : 캐글"
      ],
      "metadata": {
        "id": "Yw5mfB-1YfRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Tokenization**\n",
        "\n",
        "In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words.\n",
        "\n",
        "There are mainly two types of tokenization :\n",
        "\n",
        "- Sentence Tokenization\n",
        "- Word Tokenization"
      ],
      "metadata": {
        "id": "zZBGXubsY6lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import package\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize "
      ],
      "metadata": {
        "id": "zpux756aZRgB",
        "outputId": "27c8fb5d-8f81-407f-e8b4-699c91e64825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample text to perform our operations\n",
        "text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\""
      ],
      "metadata": {
        "id": "-vKDqW1WZcjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "id": "GowligokZeEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9c24195-f1ca-405d-8ba5-d470299714cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi, My name is Amartya Nambiar.',\n",
              " 'I am a Computer Science Engineer.',\n",
              " 'My favourite color is black']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 토큰화, 길이 출력\n",
        "words =word_tokenize(text)   #tokenized into words\n",
        "print(len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "id": "pY1VFCkVaDrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a7da09-4134-42ea-b78c-4cb19deeb3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "['Hi', ',', 'My', 'name', 'is', 'Amartya', 'Nambiar', '.', 'I', 'am', 'a', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'is', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Stopwords & Flushing them**\n",
        "\n",
        "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
      ],
      "metadata": {
        "id": "unjlrUQTaiGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords  "
      ],
      "metadata": {
        "id": "Rf5p8-7KazcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce1f032-c056-434b-99a9-fa625a08cc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# english stopword 불러오기, 15개만 확인\n",
        "stop = stopwords.words('english')\n",
        "print(len(stop))\n",
        "print(stop[:15])"
      ],
      "metadata": {
        "id": "f8kqXiktbBSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27de4bb1-ef5b-44d4-b758-c56a0fba431e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필터링을 통해 text에서 stopword 제거\n",
        "clean = [i for i in words if not i in stop]      # removing stopwords from our sample text\n",
        "print(len(clean))\n",
        "print(clean)"
      ],
      "metadata": {
        "id": "CTeQujmRbPZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa7771a-8316-4d08-89a2-b700d19ba162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "['Hi', ',', 'My', 'name', 'Amartya', 'Nambiar', '.', 'I', 'Computer', 'Science', 'Engineer', '.', 'My', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# punctuation('.', ',') 제거\n",
        "import string \n",
        "punctuations = list(string.punctuation)        \n",
        "stop += punctuations                           \n",
        "words =word_tokenize(text.lower())\n",
        "clean_lower = [i for i in words if not i in stop]\n",
        "print(len(clean_lower))\n",
        "print(clean_lower)"
      ],
      "metadata": {
        "id": "lxAH2ytMb3TY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c683c644-3c6e-4c96-f861-3ab71d5ea790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "['hi', 'name', 'amartya', 'nambiar', 'computer', 'science', 'engineer', 'favourite', 'color', 'black']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Stemming**\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
        "\n",
        "There are mainly two widely used Stemmer Algorithms:\n",
        "\n",
        "- Porter Stemmer (we'll work on this)\n",
        "- Lancaster Stem"
      ],
      "metadata": {
        "id": "yEAlzfMhcBcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "EQQuNe0McNBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ps 객체 생성 후 stemming , example 최소 3개 임의 생성 후 시도해보기\n",
        "# example1= ['helps', 'helping', 'helped']\n",
        "\n",
        "ps = PorterStemmer()         \n",
        "example = ['helps','helping','helped']   \n",
        "stemmed_example = [ps.stem(i) for i in example]\n",
        "stemmed_example"
      ],
      "metadata": {
        "id": "hp_atqYwdkR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cc0ce2-2a20-46de-cf4e-94729c382f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['help', 'help', 'help']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps.stem('happiness') # but it isn't always the best choice"
      ],
      "metadata": {
        "id": "mwMDJ3ZaduyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79de8879-86c3-49c7-c636-490bc05938b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Parts of Speech**\n",
        "\n",
        "To know what is the context of a particular word\n",
        "\n",
        "For example : Shyam is a Proper Noun, Desk is a Noun and Happy is an adjective."
      ],
      "metadata": {
        "id": "71eO1YE1dwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.corpus import movie_reviews\n",
        "text = movie_reviews.raw(\"neg/cv954_19932.txt\") "
      ],
      "metadata": {
        "id": "S2iVOPi4eEKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MihIDl4tJr5d",
        "outputId": "f04cd1ab-d110-4c08-8a90-9ac2c01b62e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply pos_tag(), print result\n",
        "print(pos_tag(word_tokenize(text)))"
      ],
      "metadata": {
        "id": "vcWgmScAedLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10c7463b-5a74-4d21-c30b-64c259a1ccbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('a', 'DT'), ('new', 'JJ'), ('entry', 'NN'), ('in', 'IN'), ('the', 'DT'), ('``', '``'), ('revisionist', 'JJ'), ('history', 'NN'), ('``', '``'), ('genre', 'NN'), ('of', 'IN'), ('filmmaking', 'NN'), (',', ','), ('dick', 'JJ'), ('suggests', 'VBZ'), ('that', 'IN'), ('two', 'CD'), ('not-too-bright', 'JJ'), ('teenage', 'NN'), ('girls', 'NNS'), ('are', 'VBP'), ('the', 'DT'), ('cause', 'NN'), ('of', 'IN'), ('the', 'DT'), ('uncovering', 'NN'), ('of', 'IN'), ('the', 'DT'), ('nation', 'NN'), (\"'s\", 'POS'), ('biggest', 'JJS'), ('presidential', 'JJ'), ('scandal', 'NN'), ('.', '.'), ('kirsten', 'VB'), ('dunst', 'JJ'), ('and', 'CC'), ('michelle', 'JJ'), ('williams', 'NNS'), ('star', 'VBP'), ('betsy', 'NN'), ('and', 'CC'), ('arlene', 'NN'), (',', ','), ('who', 'WP'), ('while', 'IN'), ('trying', 'VBG'), ('to', 'TO'), ('deliver', 'VB'), ('a', 'DT'), ('fan', 'NN'), ('letter', 'NN'), ('from', 'IN'), ('arlene', 'NN'), (\"'s\", 'POS'), ('watergate', 'JJ'), ('hotel', 'NN'), ('room', 'NN'), (',', ','), ('accidentally', 'RB'), ('stumble', 'JJ'), ('across', 'IN'), ('g', 'NN'), ('.', '.'), ('gordon', 'NN'), ('liddy', 'NN'), ('(', '('), ('played', 'VBN'), ('dead-on', 'NN'), ('by', 'IN'), ('harry', 'JJ'), ('shearer', 'NN'), (')', ')'), ('and', 'CC'), ('the', 'DT'), ('infamous', 'JJ'), ('break-in', 'NN'), ('.', '.'), ('when', 'WRB'), ('they', 'PRP'), ('recognize', 'VBP'), ('liddy', 'JJ'), ('later', 'RB'), ('on', 'IN'), ('during', 'IN'), ('a', 'DT'), ('white', 'JJ'), ('house', 'NN'), ('field', 'NN'), ('trip', 'NN'), (',', ','), ('they', 'PRP'), ('are', 'VBP'), ('ushered', 'JJ'), ('into', 'IN'), ('a', 'DT'), ('conference', 'NN'), ('room', 'NN'), (',', ','), ('questioned', 'VBD'), ('as', 'IN'), ('to', 'TO'), ('what', 'WP'), ('they', 'PRP'), ('know', 'VBP'), (',', ','), ('and', 'CC'), ('leave', 'VBP'), ('as', 'IN'), ('official', 'JJ'), ('presidential', 'JJ'), ('dog', 'NN'), ('walkers', 'NNS'), ('.', '.'), ('the', 'DT'), ('girls', 'JJ'), ('manage', 'NN'), ('to', 'TO'), ('unwittingly', 'RB'), ('uncover', 'JJ'), ('every', 'DT'), ('bit', 'NN'), ('of', 'IN'), ('the', 'DT'), ('watergate', 'NN'), ('scandal', 'NN'), ('while', 'IN'), ('performing', 'VBG'), ('their', 'PRP$'), ('duties', 'NNS'), (',', ','), ('but', 'CC'), ('have', 'VBP'), ('no', 'DT'), ('clue', 'NN'), ('as', 'IN'), ('to', 'TO'), ('what', 'WP'), ('they', 'PRP'), ('are', 'VBP'), ('getting', 'VBG'), ('involved', 'VBN'), ('with', 'IN'), ('.', '.'), ('when', 'WRB'), ('they', 'PRP'), ('discover', 'VBP'), ('that', 'IN'), ('nixon', 'NN'), ('(', '('), ('another', 'DT'), ('dead-on', 'NN'), ('performance', 'NN'), ('by', 'IN'), ('dan', 'NN'), ('hedaya', 'NN'), (',', ','), ('who', 'WP'), ('actually', 'RB'), ('favors', 'VBZ'), ('nixon', 'JJ'), ('slightly', 'RB'), (',', ','), ('unlike', 'IN'), ('anthony', 'NN'), ('hopkins', 'NNS'), (')', ')'), ('has', 'VBZ'), ('been', 'VBN'), ('abusive', 'JJ'), ('to', 'TO'), ('checkers', 'NNS'), (',', ','), ('the', 'DT'), ('presidential', 'JJ'), ('dog', 'NN'), (',', ','), ('thanks', 'NNS'), ('to', 'TO'), ('the', 'DT'), ('conversations', 'NNS'), ('that', 'IN'), ('he', 'PRP'), ('always', 'RB'), ('recorded', 'VBD'), (',', ','), ('they', 'PRP'), ('quit', 'VBP'), ('and', 'CC'), ('become', 'VBP'), ('disillusioned', 'VBN'), ('.', '.'), ('during', 'IN'), ('a', 'DT'), ('prank', 'NN'), ('phone', 'NN'), ('call', 'NN'), ('the', 'DT'), ('girls', 'NNS'), ('make', 'VBP'), ('to', 'TO'), ('woodward', 'VB'), ('and', 'CC'), ('bernstein', 'VB'), (',', ','), ('events', 'NNS'), ('are', 'VBP'), ('set', 'VBN'), ('into', 'IN'), ('motion', 'NN'), ('that', 'WDT'), ('eventually', 'RB'), ('lead', 'VBP'), ('to', 'TO'), ('the', 'DT'), ('president', 'NN'), (\"'s\", 'POS'), ('resignation', 'NN'), ('.', '.'), ('this', 'DT'), ('film', 'NN'), ('starts', 'VBZ'), ('off', 'IN'), ('promisingly', 'RB'), ('with', 'IN'), ('an', 'DT'), ('aged', 'VBN'), ('woodward', 'NN'), ('and', 'CC'), ('bernstein', 'NN'), ('arguing', 'VBG'), ('with', 'IN'), ('each', 'DT'), ('other', 'JJ'), ('on', 'IN'), ('an', 'DT'), ('obvious', 'JJ'), ('larry', 'NN'), ('king-type', 'JJ'), ('talk', 'NN'), ('show', 'NN'), ('(', '('), ('featuring', 'VBG'), ('a', 'DT'), ('cameo', 'NN'), ('by', 'IN'), ('french', 'JJ'), ('stewart', 'NN'), (')', ')'), ('about', 'IN'), ('revealing', 'VBG'), ('the', 'DT'), ('identity', 'NN'), ('of', 'IN'), ('``', '``'), ('deep', 'JJ'), ('throat', 'NN'), ('``', '``'), ('.', '.'), ('from', 'IN'), ('there', 'RB'), (',', ','), ('we', 'PRP'), ('are', 'VBP'), ('subjected', 'VBN'), ('to', 'TO'), ('bodily', 'VB'), ('function', 'NN'), ('humor', 'NN'), ('and', 'CC'), ('just', 'RB'), ('about', 'IN'), ('every', 'DT'), ('bad', 'JJ'), ('``', '``'), ('dick', 'JJ'), ('``', '``'), ('joke', 'VBD'), ('one', 'CD'), ('can', 'MD'), ('derive', 'VB'), ('from', 'IN'), ('this', 'DT'), ('type', 'NN'), ('of', 'IN'), ('supposed', 'JJ'), ('comedy', 'NN'), ('.', '.'), ('at', 'IN'), ('one', 'CD'), ('point', 'NN'), (',', ','), ('the', 'DT'), ('girls', 'NNS'), ('are', 'VBP'), ('having', 'VBG'), ('to', 'TO'), ('scream', 'VB'), ('over', 'IN'), ('a', 'DT'), ('high', 'JJ'), ('school', 'NN'), ('band', 'VBP'), ('playing', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('steps', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('lincoln', 'NN'), ('memorial', 'NN'), ('.', '.'), ('the', 'DT'), ('band', 'NN'), ('manages', 'VBZ'), ('to', 'TO'), ('stop', 'VB'), ('right', 'NN'), ('as', 'IN'), ('dunst', 'JJ'), ('screams', 'VBP'), ('``', '``'), ('you', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('stop', 'VB'), ('letting', 'VBG'), ('dick', 'JJ'), ('run', 'VB'), ('your', 'PRP$'), ('life', 'NN'), ('!', '.'), ('``', '``'), ('much', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('horror', 'NN'), ('of', 'IN'), ('everyone', 'NN'), ('standing', 'VBG'), ('within', 'IN'), ('earshot', 'JJ'), ('.', '.'), ('several', 'JJ'), ('other', 'JJ'), ('variations', 'NNS'), ('on', 'IN'), ('this', 'DT'), ('wordplay', 'NN'), ('surface', 'NN'), ('all', 'DT'), ('throughout', 'IN'), ('the', 'DT'), ('film', 'NN'), ('.', '.'), ('if', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('had', 'VBD'), ('been', 'VBN'), ('smarter', 'JJ'), ('i', 'NN'), ('would', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('less', 'RBR'), ('likely', 'JJ'), ('to', 'TO'), ('fault', 'VB'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('juvenile', 'JJ'), ('bathroom', 'NN'), ('humor', 'NN'), (',', ','), ('but', 'CC'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('not', 'RB'), ('.', '.'), ('the', 'DT'), ('film', 'NN'), ('was', 'VBD'), ('apparently', 'RB'), ('made', 'VBN'), ('for', 'IN'), ('relatively', 'RB'), ('younger', 'JJR'), ('people', 'NNS'), ('because', 'IN'), ('every', 'DT'), ('major', 'JJ'), ('player', 'NN'), ('in', 'IN'), ('the', 'DT'), ('watergate', 'NN'), ('scandal', 'NN'), ('is', 'VBZ'), ('introduced', 'VBN'), ('and', 'CC'), ('shoved', 'VBN'), ('down', 'RP'), ('the', 'DT'), ('audience', 'NN'), (\"'s\", 'POS'), ('throat', 'NN'), ('in', 'IN'), ('the', 'DT'), ('least', 'JJS'), ('subtle', 'JJ'), ('way', 'NN'), ('possible', 'JJ'), ('.', '.'), ('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB'), ('recall', 'VB'), ('oliver', 'RP'), ('stone', 'NN'), (\"'s\", 'POS'), ('nixon', 'JJ'), ('having', 'VBG'), ('to', 'TO'), ('pander', 'VB'), ('to', 'TO'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('audience', 'NN'), (',', ','), ('but', 'CC'), ('of', 'IN'), ('course', 'NN'), ('that', 'IN'), ('film', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('a', 'DT'), ('comedy', 'NN'), ('aimed', 'VBN'), ('squarely', 'RB'), ('at', 'IN'), ('a', 'DT'), ('13-20', 'JJ'), ('year-old', 'JJ'), ('film', 'NN'), ('going', 'VBG'), ('audience', 'NN'), ('.', '.'), ('the', 'DT'), ('only', 'JJ'), ('redeeming', 'JJ'), ('thing', 'NN'), ('about', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('is', 'VBZ'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('remarkable', 'JJ'), ('supporting', 'VBG'), ('cast', 'NN'), ('.', '.'), ('i', 'NN'), ('wanted', 'VBD'), ('to', 'TO'), ('see', 'VB'), ('more', 'JJR'), ('of', 'IN'), ('ferrell', 'NN'), ('and', 'CC'), ('mcculloch', 'NN'), (\"'s\", 'POS'), ('woodward', 'NN'), ('and', 'CC'), ('bernstein', 'NN'), ('.', '.'), ('those', 'DT'), ('two', 'CD'), ('characters', 'NNS'), ('are', 'VBP'), ('the', 'DT'), ('sole', 'JJ'), ('basis', 'NN'), ('for', 'IN'), ('my', 'PRP$'), ('rating', 'NN'), ('.', '.'), ('i', 'JJ'), ('wish', 'NN'), ('they', 'PRP'), ('had', 'VBD'), ('been', 'VBN'), ('given', 'VBN'), ('more', 'RBR'), ('screen', 'JJ'), ('time', 'NN'), (',', ','), ('but', 'CC'), ('unfortunately', 'RB'), (',', ','), ('they', 'PRP'), ('are', 'VBP'), ('only', 'RB'), ('relegated', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('final', 'JJ'), ('half-hour', 'NN'), ('.', '.'), ('their', 'PRP$'), ('constant', 'JJ'), ('bickering', 'NN'), ('and', 'CC'), ('fighting', 'NN'), ('over', 'IN'), ('trying', 'VBG'), ('to', 'TO'), ('get', 'VB'), ('the', 'DT'), ('story', 'NN'), ('are', 'VBP'), ('a', 'DT'), ('major', 'JJ'), ('highlight', 'NN'), (',', ','), ('especially', 'RB'), ('mcculloch', 'VBP'), (\"'s\", 'POS'), ('constant', 'JJ'), ('thwarting', 'NN'), ('of', 'IN'), ('ferrell', 'NN'), (\"'s\", 'POS'), ('attempts', 'NNS'), ('to', 'TO'), ('gather', 'VB'), ('information', 'NN'), ('from', 'IN'), ('the', 'DT'), ('girls', 'NNS'), ('(', '('), ('who', 'WP'), (',', ','), ('in', 'IN'), ('the', 'DT'), ('course', 'NN'), ('of', 'IN'), ('the', 'DT'), ('narrative', 'JJ'), ('are', 'VBP'), ('revealed', 'VBN'), ('as', 'IN'), ('deep', 'JJ'), ('throat', 'NN'), (',', ','), ('so', 'RB'), ('named', 'JJ'), ('thanks', 'NNS'), ('to', 'TO'), ('an', 'DT'), ('ill', 'NN'), ('planned', 'VBN'), ('trip', 'NN'), ('to', 'TO'), ('a', 'DT'), ('porno', 'NN'), ('theater', 'NN'), ('by', 'IN'), ('betsy', 'NN'), (\"'s\", 'POS'), ('brother', 'NN'), (')', ')'), ('.', '.'), ('the', 'DT'), ('other', 'JJ'), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('cast', 'NN'), ('are', 'VBP'), ('excellent', 'VBN'), ('in', 'IN'), ('their', 'PRP$'), ('portrayals', 'NNS'), ('of', 'IN'), ('their', 'PRP$'), ('particular', 'JJ'), ('characters', 'NNS'), (',', ','), ('but', 'CC'), ('are', 'VBP'), ('given', 'VBN'), ('nothing', 'NN'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('.', '.'), ('i', 'JJ'), (\"'d\", 'MD'), ('like', 'VB'), ('to', 'TO'), ('see', 'VB'), ('the', 'DT'), ('same', 'JJ'), ('cast', 'NN'), ('portray', 'NN'), ('these', 'DT'), ('characters', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('script', 'NN'), ('more', 'RBR'), ('suited', 'JJ'), ('towards', 'NNS'), ('their', 'PRP$'), ('comedic', 'JJ'), ('abilities', 'NNS'), ('.', '.'), ('as', 'IN'), ('for', 'IN'), ('the', 'DT'), ('two', 'CD'), ('leads', 'NNS'), (',', ','), ('dunst', 'NN'), ('and', 'CC'), ('williams', 'NNS'), ('can', 'MD'), ('definitely', 'RB'), ('do', 'VB'), ('better', 'JJR'), ('.', '.'), ('they', 'PRP'), ('come', 'VBP'), ('off', 'RP'), ('as', 'IN'), ('what', 'WP'), ('could', 'MD'), ('best', 'RB'), ('be', 'VB'), ('described', 'VBN'), ('as', 'IN'), ('romy', 'NN'), ('and', 'CC'), ('michele', 'NN'), (':', ':'), ('the', 'DT'), ('early', 'JJ'), ('years', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('particular', 'JJ'), ('film', 'NN'), (',', ','), ('a', 'DT'), ('highly', 'RB'), ('dubious', 'JJ'), ('distinction', 'NN'), ('at', 'IN'), ('best', 'JJS'), ('.', '.'), ('stay', 'VB'), ('through', 'IN'), ('the', 'DT'), ('first', 'JJ'), ('half', 'NN'), ('of', 'IN'), ('the', 'DT'), ('end', 'NN'), ('credits', 'VBZ'), ('though', 'IN'), (',', ','), ('to', 'TO'), ('see', 'VB'), ('an', 'DT'), ('interesting', 'JJ'), ('scene', 'NN'), ('involving', 'VBG'), ('dunst', 'NN'), ('and', 'CC'), ('williams', 'NNS'), ('suggestively', 'RB'), ('sucking', 'VBG'), ('on', 'IN'), ('lollipops', 'NNS'), ('emblazoned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('title', 'NN'), ('of', 'IN'), ('the', 'DT'), ('movie', 'NN'), ('.', '.'), ('an', 'DT'), ('excellent', 'JJ'), ('idea', 'NN'), ('marred', 'VBN'), ('by', 'IN'), ('poor', 'JJ'), ('execution', 'NN'), (',', ','), ('dick', 'NN'), ('could', 'MD'), ('have', 'VB'), ('been', 'VBN'), ('a', 'DT'), ('great', 'JJ'), ('movie', 'NN'), ('.', '.'), ('less', 'JJR'), ('of', 'IN'), ('the', 'DT'), ('juvenile', 'NN'), ('humor', 'NN'), ('and', 'CC'), ('more', 'JJR'), ('of', 'IN'), ('the', 'DT'), ('smarter', 'NN'), ('comedy', 'NN'), ('displayed', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('woodward', 'NN'), ('and', 'CC'), ('bernstein', 'NN'), ('scenes', 'NNS'), (',', ','), ('could', 'MD'), ('have', 'VB'), ('made', 'VBN'), ('this', 'DT'), ('film', 'NN'), ('a', 'DT'), ('wonderful', 'JJ'), ('satire', 'NN'), ('of', 'IN'), ('the', 'DT'), ('nixon', 'JJ'), ('presidency', 'NN'), ('as', 'IN'), ('seen', 'VBN'), ('through', 'IN'), ('the', 'DT'), ('eyes', 'NNS'), ('of', 'IN'), ('two', 'CD'), ('naive', 'JJ'), ('fifteen', 'JJ'), ('year', 'NN'), ('olds', 'NNS'), ('.', '.'), ('as', 'IN'), ('it', 'PRP'), ('stands', 'VBZ'), ('though', 'IN'), (',', ','), ('dick', 'JJ'), ('offers', 'NNS'), ('nothing', 'NN'), ('but', 'CC'), ('what', 'WP'), ('filmmaker', 'NN'), ('kevin', 'NN'), ('smith', 'NN'), ('so', 'RB'), ('accurately', 'RB'), ('defines', 'VBZ'), ('as', 'IN'), ('``', '``'), ('dick', 'NN'), ('and', 'CC'), ('poopie', 'NN'), ('``', '``'), ('jokes', 'NNS'), ('.', '.'), ('and', 'CC'), ('that', 'IN'), (',', ','), ('to', 'TO'), ('me', 'PRP'), (',', ','), ('does', 'VBZ'), ('not', 'RB'), ('make', 'VB'), ('a', 'DT'), ('funny', 'JJ'), ('movie', 'NN'), ('.', '.'), ('[', 'JJ'), ('pg-13', 'JJ'), (']', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Lemmatization**\n",
        "\n",
        "PorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data.\n",
        "\n",
        "Stemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ],
      "metadata": {
        "id": "NrFml-IJepQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import package\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "xWF0Ibznetk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e906f64d-6132-4dcb-fc4d-58721a13e083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize 'believes', 'happiness'\n",
        "lem = WordNetLemmatizer()\n",
        "lem.lemmatize('believes')"
      ],
      "metadata": {
        "id": "C-abWCqffiwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "14de2ac8-d012-472d-815b-c3678605735e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'belief'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem = WordNetLemmatizer()\n",
        "lem.lemmatize('happiness')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4Tk5Gqs7LFSr",
        "outputId": "b61364ac-ed98-41ba-fa83-bf4a57e7950c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('believes', pos='a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aR2bn9OeIbCx",
        "outputId": "547de0b7-e993-4430-a901-b7dd8683c57c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'believes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('happiness', pos='a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YlGN0eL3LI-k",
        "outputId": "9d587f41-0a52-499b-fb3f-d8fa8b0edb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('believes', pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UKyPqaH6IhWP",
        "outputId": "d0cfedf5-b237-4d60-f58f-65c4a0e5282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'believe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lem.lemmatize('happiness', pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QV--mOEfLMMW",
        "outputId": "8b27f7a6-0685-4173-8605-ffa159ad831a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'happiness'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}